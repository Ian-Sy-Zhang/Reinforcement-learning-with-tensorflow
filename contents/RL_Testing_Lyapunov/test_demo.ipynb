{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import Util\n",
    "sys.path.append('..')\n",
    "import Env_shy\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import TimeLimit \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../9_Deep_Deterministic_Policy_Gradient_DDPG')\n",
    "import DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, total_time, env, n):\n",
    "    env.reset()\n",
    "    cumulated_punish = 0\n",
    "    for i in range(total_time):\n",
    "        env.state = np.random.uniform(-1, 1, size=(2, 1))\n",
    "        state = env.state\n",
    "        ideal_action = - env.K @ state\n",
    "        action = model.predict(state, deterministic=True)\n",
    "        cumulated_punish += abs(action[0] - ideal_action).sum()\n",
    "    return cumulated_punish/total_time\n",
    "\n",
    "def evaluate_new(actor, total_time, env, n):\n",
    "    env.reset()\n",
    "    cumulated_punish = 0\n",
    "    for i in range(total_time):\n",
    "        env.state = np.random.uniform(-1, 1, size=(2, 1))\n",
    "        state = env.state\n",
    "        ideal_action = -env.K @ state\n",
    "        action = actor.choose_action(state)\n",
    "        cumulated_punish += abs(action[0] - ideal_action).sum()\n",
    "    return cumulated_punish/total_time\n",
    "\n",
    "def generate_state_transition_matix(n, m):\n",
    "    is_A_stable = True\n",
    "    while(is_A_stable):\n",
    "        A = np.random.rand(n, n)\n",
    "        eigenvalues = np.linalg.eigvals(A)\n",
    "        is_A_stable = all(np.abs(eigenvalues) < 1)\n",
    "    B = np.random.rand(n, m)\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to use different envs run on same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = []\n",
    "cumulated_punish_record = []\n",
    "n = 2\n",
    "m = 2\n",
    "log_file = \"./diff_env_a2c/\"\n",
    "total_timesteps = 50000\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    is_A_stable = True\n",
    "    while(is_A_stable):\n",
    "        A = np.random.rand(n, n)\n",
    "        eigenvalues = np.linalg.eigvals(A)\n",
    "        is_A_stable = all(np.abs(eigenvalues) < 1)\n",
    "    B = np.random.rand(n, m)\n",
    "\n",
    "    record.append((A, B))\n",
    "    env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "    env.reset()\n",
    "    DDPG.train(env=env)\n",
    "\n",
    "    cumulated_punish_record.append(evaluate(DDPG.actor, 1000, env, n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = []\n",
    "cumulated_punish_record = []\n",
    "n = 2\n",
    "m = 2\n",
    "log_file = \"./diff_env_a2c/\"\n",
    "total_timesteps = 50000\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    is_A_stable = True\n",
    "    while(is_A_stable):\n",
    "        A = np.random.rand(n, n)\n",
    "        eigenvalues = np.linalg.eigvals(A)\n",
    "        is_A_stable = all(np.abs(eigenvalues) < 1)\n",
    "    B = np.random.rand(n, m)\n",
    "\n",
    "    record.append((A, B))\n",
    "    env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "    env.reset()\n",
    "    model = sb3.A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    cumulated_punish_record.append(evaluate(model, 1000, env, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to use same env run on same model for multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulated_punish_record = []\n",
    "n = 2\n",
    "m = 2\n",
    "log_file = \"./diff_env_a2c/\"\n",
    "total_timesteps = 50000\n",
    "\n",
    "is_A_stable = True\n",
    "while(is_A_stable):\n",
    "    A = np.random.rand(n, n)\n",
    "    eigenvalues = np.linalg.eigvals(A)\n",
    "    is_A_stable = all(np.abs(eigenvalues) < 1)\n",
    "B = np.random.rand(n, m)\n",
    "env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "\n",
    "for i in range(1):\n",
    "    env.reset()\n",
    "    model = sb3.A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    cumulated_punish_record.append(evaluate(model, 1000, env, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to run different max_episode_steps on same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 2\n",
    "log_file = \"./diff_env_a2c/\"\n",
    "total_timesteps = 50000\n",
    "\n",
    "is_A_stable = True\n",
    "while(is_A_stable):\n",
    "    A = np.random.rand(n, n)\n",
    "    eigenvalues = np.linalg.eigvals(A)\n",
    "    is_A_stable = all(np.abs(eigenvalues) < 1)\n",
    "B = np.random.rand(n, m)\n",
    "\n",
    "cumulated_punish_record = []\n",
    "for i in [1, 2, 5, 10, 20, 50, 100]:\n",
    "    env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\"), max_episode_steps=i)\n",
    "    env.reset()\n",
    "    model = sb3.A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    \n",
    "    cumulated_punish_record.append(evaluate(model, 3000, env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to use specific A, B to create  env and train on same model for multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.36315674, 0.06881483], [0.83578281, 0.42088278]])\n",
    "B = np.array([[0.96880357, 0.72537713], [0.14729658, 0.63892631]]) \n",
    "cumulated_punish_record = []\n",
    "n = 2\n",
    "m = 2\n",
    "log_file = \"./diff_env_a2c/\"\n",
    "total_timesteps = 50000\n",
    "\n",
    "for i in range(1):\n",
    "    env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\"), max_episode_steps=1)\n",
    "    env.reset()\n",
    "    model = sb3.A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    cumulated_punish_record.append(evaluate(model, 1000, env, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to compare ideal_action with action given by trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "cumulated_punish = 0\n",
    "total_timesteps = 3000\n",
    "\n",
    "for i in range(total_timesteps):\n",
    "    env.state = np.random.rand(2, 1)\n",
    "    state = env.state\n",
    "    ideal_action = - env.K @ state\n",
    "    action = model.predict(state, deterministic=True)\n",
    "    cumulated_punish += abs(action[0] - ideal_action).sum()\n",
    "cumulated_punish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to compare ideal_action and X_star_k_plus_1 with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 10000\n",
    "\n",
    "for i in range(total_timesteps):\n",
    "    env.state = np.random.rand(2, 1)\n",
    "    state = env.state\n",
    "    ideal_action = - env.K @ state\n",
    "    action = model.predict(state, deterministic=True)[0]\n",
    "    X_star_k_plus_1 = (env.Acl @ state).astype(np.float32)\n",
    "    X_k_plus_1 = (A @ state + B @ action).astype(np.float32)\n",
    "    print(action, ideal_action, X_k_plus_1, X_star_k_plus_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to compare different algorithms on same env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 2\n",
    "log_file = \"./diff_env_a2c/\"\n",
    "total_timesteps = 50000\n",
    "\n",
    "is_A_stable = True\n",
    "while(is_A_stable):\n",
    "    A = np.random.rand(n, n)\n",
    "    eigenvalues = np.linalg.eigvals(A)\n",
    "    is_A_stable = all(np.abs(eigenvalues) < 1)\n",
    "B = np.random.rand(n, m)\n",
    "env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "\n",
    "algorithms = ['A2C', 'DDPG', 'PPO', 'SAC']\n",
    "record_test = []\n",
    "for algorithm in algorithms:\n",
    "    if algorithm == 'A2C':\n",
    "        model = sb3.A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)  \n",
    "    elif algorithm == 'DDPG':\n",
    "        model = sb3.DDPG(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    elif algorithm == 'PPO':\n",
    "        model = sb3.PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    elif algorithm == 'SAC':\n",
    "        model = sb3.SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    elif algorithm == 'TD3':\n",
    "        model = sb3.TD3(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_file)\n",
    "    else:\n",
    "        print(\"error: \", algorithm)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    record_test.append(evaluate(model, 3000, env, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to draw 3D graph(3D: x, y, reward) given A, B, Acl..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateReward(X_star_k_plus_1, X_k_plus_1, P):\n",
    "        punish = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                if i == j:\n",
    "                    diff = pow(X_k_plus_1[i], 2) - pow(X_star_k_plus_1[i], 2)\n",
    "                    punish += abs(P[i, j] * diff)\n",
    "                else:\n",
    "                    diff = X_k_plus_1[i] * X_k_plus_1[j] - X_star_k_plus_1[i] * X_star_k_plus_1[j]\n",
    "                    punish += abs(2 * P[i, j] * diff)\n",
    "        return punish "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "y = np.linspace(-1, 1, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "A, B = generate_state_transition_matix(2, 2)\n",
    "env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        x_val, y_val = X[i, j], Y[i, j]\n",
    "        action = np.array([[x_val], [y_val]])\n",
    "        X_k_plus_1 = (A @ state + B @ action).astype(np.float32)\n",
    "        X_star_k_plus_1 = np.ones_like(X_k_plus_1)\n",
    "        z_val = calculateReward(X_k_plus_1, X_star_k_plus_1, env.P)\n",
    "        Z[i, j] = z_val[0]\n",
    "\n",
    "fig = plt.figure(dpi=100)\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis') \n",
    "ax.view_init(elev=30, azim=45)\n",
    "\n",
    "ax.set_xlabel('a_1')\n",
    "ax.set_ylabel('b_1')\n",
    "ax.set_zlabel('potential-based reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to draw field given env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "y = np.linspace(-1, 1, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "n = 2\n",
    "m = 2\n",
    "\n",
    "A, B = generate_state_transition_matix(2, 2)\n",
    "env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "\n",
    "num_points = 30\n",
    "side_length = 1\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_points))\n",
    "initial_points = Util.generate_X0_uniform(num_points, side_length)\n",
    "fig, ax = plt.subplots()\n",
    "x_values, y_values, U, V = Util.generate_field_grid(env.Acl, initial_points)\n",
    "ax.quiver(x_values, y_values, U, V, color=colors[0], angles='xy',\n",
    "            scale_units='xy', scale=3, width=.005)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to draw field given trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "y = np.linspace(-1, 1, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "n = 2\n",
    "m = 2\n",
    "\n",
    "A, B = generate_state_transition_matix(2, 2)\n",
    "env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "#train u model\n",
    "model = sb3.A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(10000)\n",
    "\n",
    "num_points = 30\n",
    "side_length = 1\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_points))\n",
    "initial_points = Util.generate_X0_uniform(num_points, side_length)\n",
    "fig, ax = plt.subplots()\n",
    "x_values, y_values, U, V = Util.generate_field_grid_for_model(model, initial_points, env)\n",
    "ax.quiver(x_values, y_values, U, V, color=colors[0], angles='xy',\n",
    "            scale_units='xy', scale=3, width=.005)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to draw trajectory given A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.8, -0.5], [0.3, 0.9]]) \n",
    "num_points = 5\n",
    "num_steps = 50\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_points))\n",
    "initial_points = Util.generate_X0_circle(num_points, (0, 0),5)\n",
    "fig, ax = plt.subplots()\n",
    "for i, X0 in enumerate(initial_points):\n",
    "    x_values, y_values, U, V = Util.generate_trajectory(A, X0, num_steps)\n",
    "    ax.quiver(x_values, y_values, U, V, color=colors[i], angles='xy',\n",
    "            scale_units='xy', scale=2, width=.005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to draw trajectory given our env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 2\n",
    "A = (np.random.rand(n, n) - 1) * 10\n",
    "B = (np.random.rand(n, m) - 1) * 10\n",
    "Q = np.eye(n)\n",
    "R = np.eye(m) * 100\n",
    "K, P = Util.findPK(A, B, Q, R)\n",
    "Acl = A - B @ K\n",
    "\n",
    "A = Acl\n",
    "num_points = 20\n",
    "num_steps = 40\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_points))\n",
    "initial_points = Util.generate_X0_circle(num_points, (0, 0), 5)\n",
    "fig, ax = plt.subplots()\n",
    "for i, X0 in enumerate(initial_points):\n",
    "    x_values, y_values, U, V = Util.generate_trajectory(A, X0, num_steps)\n",
    "    ax.quiver(x_values, y_values, U, V, color=colors[i], angles='xy',\n",
    "            scale_units='xy', scale=1, width=.005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo how to draw trajectory given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 2\n",
    "A, B = generate_state_transition_matix(n, m)\n",
    "env = TimeLimit(Env_shy.CustomEnv(A, B, n, m, reward_type=\"Shy\", test=False), max_episode_steps=1)\n",
    "Acl = env.Acl\n",
    "model = sb3.A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 10\n",
    "num_steps = 10\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_points))\n",
    "initial_points = Util.generate_X0_circle(num_points, (0, 0), 1)\n",
    "fig, ax = plt.subplots()\n",
    "for i, X0 in enumerate(initial_points):\n",
    "    x_values, y_values, U, V = Util.generate_trajectory_for_model(env, model, X0, num_steps)\n",
    "    ax.quiver(x_values, y_values, U, V, color=colors[i], angles='xy',\n",
    "            scale_units='xy', scale=1, width=.005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB_Testing_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
